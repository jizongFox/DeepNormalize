        # if self._pretraining_config.pretrained:
        #     generator_epoch, discriminator_epoch, segmenter_epoch = 0, 0, 0
        #     if "generator" in self._pretraining_config.model_paths:
        #         generator_epoch = self._generator_trainer.restore_from_checkpoint(
        #             self._pretraining_config.model_paths["generator"])
        #     if "discriminator" in self._pretraining_config.model_paths:
        #         discriminator_epoch = self._discriminator_trainer.restore_from_checkpoint(
        #             self._pretraining_config.model_paths["discriminator"])
        #     if "segmenter" in self._pretraining_config.model_paths:
        #         segmenter_epoch = self._segmenter_trainer.restore_from_checkpoint(
        #             self._pretraining_config.model_paths["segmenter"])
        #     # Get the highest epoch from checkpoints to restart training.
        #     epochs = np.array([generator_epoch, discriminator_epoch, segmenter_epoch])
        #     max_epoch = epochs.max()
        #     self._epoch = torch.Tensor().new([max_epoch])
        #
        #     # Use a barrier() to make sure that all processes have finished reading the checkpoints.
        #     torch.distributed.barrier()