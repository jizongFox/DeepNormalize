models:
  Segmenter:
    name: "UNet3D"
    type: "UNet3D"
    params:
      feature_maps: 64
      in_channels: 1
      out_channels: 4
      num_levels: 4
      conv_kernel_size: 3
      pool_kernel_size: 2
      pooling_type: "MaxPool3d"
      num_groups: 8
      padding: !!python/tuple [1, 1, 1, 1, 1, 1]
      activation: "LeakyReLU"
      interpolation: True
      scale_factor: !!python/tuple [2, 2, 2] # Used as the multiplier for the image H/W/D in torch.nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation from the corresponding encoder.
    optimizer:
      type: "FusedSGD"
      max_grad_norm: 5.0
      params:
        lr: 0.0005
        momentum: 0.9
        weight_decay: 0.1
    scheduler:
      type: "ReduceLROnPlateau"
      params:
        mode: "min"
        factor: 0.1
        patience: 3
    criterion:
      type: "DiceLoss"
      params:
        reduction: "mean"
        ignore_index: -100
    metric:
      type: "Dice"
      params:
        num_classes: 4
        reduction: "mean"
        ignore_index: 0
        average: !!null

dataset:
  iSEG:
    path: "/mnt/md0/Data/Preprocessed/iSEG/Patches/Normalized"
    validation_split: 0.2
    training:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]
    validation:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]

training:
  batch_size: 24
  nb_epochs: 50

visdom:
  server: "10.0.3.9"
  port: "8097"
  env: "deepNormalize_exp_home_segmenter_only_iSEG"

logger:
  path: "/tmp/ml/tests"
  log_after_iterations: 50

