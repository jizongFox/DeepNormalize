models:
  Generator:
    name: "UNet3D"
    type: "UNet3D"
    params:
      feature_maps: 64
      in_channels: 1
      out_channels: 1
      num_levels: 4
      conv_kernel_size: 3
      pool_kernel_size: 2
      pooling_type: "MaxPool3d"
      num_groups: 8
      padding: !!python/tuple [1, 1, 1, 1, 1, 1]
      activation: "LeakyReLU"
      interpolation: True
      scale_factor: !!python/tuple [2, 2, 2] # Used as the multiplier for the image H/W/D in torch.nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation from the corresponding encoder.
    optimizer:
      type: "FusedSGD"
      params:
        lr: 0.00005
        momentum: 0.9
        weight_decay: 0.1
    scheduler:
      type: "ReduceLROnPlateau"
      params:
        mode: "min"
        factor: 0.1
        patience: 1
    criterion:
      type: "MSELoss"
      params:
    metric:
      type: "Accuracy"
      params:
  Segmenter:
    name: "UNet3D"
    type: "UNet3D"
    params:
      feature_maps: 64
      in_channels: 1
      out_channels: 4
      num_levels: 4
      conv_kernel_size: 3
      pool_kernel_size: 2
      pooling_type: "MaxPool3d"
      num_groups: 8
      padding: !!python/tuple [1, 1, 1, 1, 1, 1]
      activation: "LeakyReLU"
      interpolation: True
      scale_factor: !!python/tuple [2, 2, 2] # Used as the multiplier for the image H/W/D in torch.nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation from the corresponding encoder.
    optimizer:
      type: "FusedSGD"
      params:
        lr: 0.001
        momentum: 0.9
        weight_decay: 0.1
    scheduler:
      type: "ReduceLROnPlateau"
      params:
        mode: "min"
        factor: 0.1
        patience: 3
    criterion:
      type: "DiceLoss"
      params:
        reduction: "mean"
        ignore_index: 0
    metric:
      type: "Dice"
      params:
        num_classes: 4
        reduction: "mean"
        ignore_index: 0
        average: !!null

dataset:
  iSEG:
    path: "data/iSEG"
    validation_split: 0.2
    training:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]
    validation:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]
  MRBrainS:
    path: "data/MRBrainS_2013"
    validation_split: 0.2
    training:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]
    validation:
      patch_size: [1, 32, 32, 32]
      step: [1, 8, 8, 8]

training:
  batch_size: 12
  nb_epochs: 100
  patience_segmentation: 3
  variables:
    lambda: 0.5
    alpha: 1.0
    train_generator_every_n_steps: 5
    clip_value: 0.01

visdom:
  server: "pldelisle.no-ip.info"
  port: "8097"
  env: "deepNormalize_exp_school_generator_segmenter"

logger:
  path: "/tmp/ml/tests"
  log_after_iterations: 50

